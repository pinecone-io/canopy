# ===========================================================
#            Configuration file for Canopy Server
# ===========================================================
# -------------------------------------------------------------------------------------------
# Tokenizer configuration
# Use LLamaTokenizer from HuggingFace with the relevant OSS model (e.g. LLama2)
# -------------------------------------------------------------------------------------------
tokenizer:
  type: LlamaTokenizer                 # Options: [OpenAITokenizer, LlamaTokenizer]
  params:
    model_name: hf-internal-testing/llama-tokenizer
# -------------------------------------------------------------------------------------------
# Chat engine configuration
# Use Anyscale Endpoint as the open source LLM provider
# You can find the list of supported LLM at https://docs.endpoints.anyscale.com/category/supported-models
# -------------------------------------------------------------------------------------------
chat_engine:
  params:
    max_prompt_tokens: 2048             # The maximum number of tokens to use for input prompt to the LLM.
  llm: &llm
    type: AnyscaleLLM                     # Options: [OpenAILLM, AnyscaleLLM]
    params:
      model_name: meta-llama/Llama-2-7b-chat-hf         # The name of the model to use.

  # --------------------------------------------------------------------
  # Configuration for the QueryBuilder subcomponent of the chat engine.
  # Since Anyscale's LLM endpoint currently doesn't support function calling, we will use the LastMessageQueryGenerator
  # --------------------------------------------------------------------
  query_builder:
    type: CondensedQueryGenerator     # Options: [FunctionCallingQueryGenerator, LastMessageQueryGenerator]

  # -------------------------------------------------------------------------------------------------------------
  # ContextEngine configuration
  # -------------------------------------------------------------------------------------------------------------
  context_engine:
    # -----------------------------------------------------------------------------------------------------------
    # KnowledgeBase configuration
    # -----------------------------------------------------------------------------------------------------------
    knowledge_base:
      # --------------------------------------------------------------------------
      # Configuration for the RecordEncoder subcomponent of the knowledge base.
      # Use Anyscale's Embedding endpoint for dense encoding
      # --------------------------------------------------------------------------
      record_encoder:
        type: AnyscaleRecordEncoder       # Options: [OpenAIRecordEncoder, AnyscaleRecordEncoder]
        params:
          model_name:                   # The name of the model to use for encoding
            thenlper/gte-large
          batch_size: 100               # The number of document chunks to encode in each call to the encoding model
