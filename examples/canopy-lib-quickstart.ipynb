{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone Canopy library quick start notebook\n",
    "\n",
    "**Canopy** is a Sofware Development Kit (SDK) for AI applications. Canopy allows you to test, build and package Retrieval Augmented Applications with Pinecone Vector Database. \n",
    "\n",
    "This notebook introduce the quick start steps for working with Canopy library. You can find more details about this project and advanced use in the project [documentaion](../README.md).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "install canopy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU git+ssh://git@github.com/pinecone-io/canopy.git@dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Canopy uses Pinecone and OpenAI so we need to configure the related API keys.\n",
    "\n",
    "To get Pinecone free trial API key and environment register or log into your Pinecone account in the [console](https://app.pinecone.io/). You can access your API key from the \"API Keys\" section in the sidebar of your dashboard, and find the environment name next to it.\n",
    "\n",
    "You can find your free trial OpenAI API key [here](https://platform.openai.com/account/api-keys). You might need to login or register to OpenAI services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.environ.get('PINECONE_API_KEY') or 'YOUR_PINECONE_API_KEY'\n",
    "os.environ[\"PINECONE_ENVIRONMENT\"] = os.environ.get('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get('OPENAI_API_KEY') or 'OPENAI_API_KEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Documentation Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load a crawl of from 25/10/23 of pinecone docs [website](https://docs.pinecone.io/docs/).\n",
    "\n",
    "We will use this data to demonstrate how to build a RAG pipepline to answer questions about Pinecone DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728aeea1-1dcf-5d0a-91f2-ecccd4dd4272</td>\n",
       "      <td># Scale indexes\\n\\n[Suggest Edits](/edit/scali...</td>\n",
       "      <td>https://docs.pinecone.io/docs/scaling-indexes</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'scaling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f19f269-171f-5556-93f3-a2d7eabbe50f</td>\n",
       "      <td># Understanding organizations\\n\\n[Suggest Edit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/organizations</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2a71cb3-5148-5090-86d5-7f4156edd7cf</td>\n",
       "      <td># Manage datasets\\n\\n[Suggest Edits](/edit/dat...</td>\n",
       "      <td>https://docs.pinecone.io/docs/datasets</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'datasets'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1dafe68a-2e78-57f7-a97a-93e043462196</td>\n",
       "      <td># Architecture\\n\\n[Suggest Edits](/edit/archit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/architecture</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd</td>\n",
       "      <td># Moving to production\\n\\n[Suggest Edits](/edi...</td>\n",
       "      <td>https://docs.pinecone.io/docs/moving-to-produc...</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'moving-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  728aeea1-1dcf-5d0a-91f2-ecccd4dd4272   \n",
       "1  2f19f269-171f-5556-93f3-a2d7eabbe50f   \n",
       "2  b2a71cb3-5148-5090-86d5-7f4156edd7cf   \n",
       "3  1dafe68a-2e78-57f7-a97a-93e043462196   \n",
       "4  8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd   \n",
       "\n",
       "                                                text  \\\n",
       "0  # Scale indexes\\n\\n[Suggest Edits](/edit/scali...   \n",
       "1  # Understanding organizations\\n\\n[Suggest Edit...   \n",
       "2  # Manage datasets\\n\\n[Suggest Edits](/edit/dat...   \n",
       "3  # Architecture\\n\\n[Suggest Edits](/edit/archit...   \n",
       "4  # Moving to production\\n\\n[Suggest Edits](/edi...   \n",
       "\n",
       "                                              source  \\\n",
       "0      https://docs.pinecone.io/docs/scaling-indexes   \n",
       "1        https://docs.pinecone.io/docs/organizations   \n",
       "2             https://docs.pinecone.io/docs/datasets   \n",
       "3         https://docs.pinecone.io/docs/architecture   \n",
       "4  https://docs.pinecone.io/docs/moving-to-produc...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'created_at': '2023_10_25', 'title': 'scaling...  \n",
       "1  {'created_at': '2023_10_25', 'title': 'organiz...  \n",
       "2  {'created_at': '2023_10_25', 'title': 'datasets'}  \n",
       "3  {'created_at': '2023_10_25', 'title': 'archite...  \n",
       "4  {'created_at': '2023_10_25', 'title': 'moving-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_parquet(\"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in this dataset represents a single page in Pinecone's documentation. Each row contatins a unique id, the raw text of the page in markdown language, the url of the page as \"source\" and some metadata. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init a Tokenizer\n",
    "\n",
    "\n",
    "Many of Canopy's components are using tokenization, which is a process that splits text into tokens - basic units of text (like word or sub-words) that are used for processing. Therefore, Canopy uses a singleton `Tokenizer` object which needs to be initialized once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "Tokenizer.initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initilizing the global object, we can simply create an instance from anywhere in our code, without providing any parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' world', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.tokenize(\"Hello world!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a KnowledgBase to store our data for search\n",
    "\n",
    "`KnowledgeBase` is an object that is responsible for storing and query data. It holds a connection to a single Pinecone index and provides a simple API to insert, delete and search textual documents.\n",
    "\n",
    "During an upsert, the KnowledgeBase divides the text into smaller chunks, transforms them into vector embeddings, and then upsert these vectors in the underlying Pinecone index. When querying, it converts the textual input into a vector and excute the queries against the underlying index to retrieve the top-k most closely matched chunks.\n",
    "\n",
    "Here we create a `KnowledgeBase` with our desired index name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base import KnowledgeBase\n",
    "\n",
    "INDEX_NAME = \"my-index\"\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a new index in Pinecone, if it's not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.create_canopy_index(indexed_fields=[\"title\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the index created in Pinecone's [console](https://app.pinecone.io/)\n",
    "\n",
    "next time we would like to init a knowledge base instance to this index, we can simply call the connect method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(index_name=INDEX_NAME)\n",
    "kb.connect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Note: a knowledge base must be connected to an index before excuting any operation. You should call `kb.connect()` to connect  an existing index or call `kb.create_canopy_index(INDEX_NANE)` before calling any other method of the KB "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert data to our KnowledgBase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert our dataset to list of `Document` objects\n",
    "\n",
    "Each document object can hold id, text, source and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import Document\n",
    "\n",
    "example_docs = [Document(id=\"1\",\n",
    "                      text=\"This is text for example\",\n",
    "                      source=\"https://url.com\"),\n",
    "                Document(id=\"2\",\n",
    "                        text=\"this is another text\",\n",
    "                        source=\"https://another-url.com\",\n",
    "                        metadata={\"my-key\": \"my-value\"})]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily the columns in our dataset fits this scehma, so we can use a simple iteration to prepare our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(**row) for _, row in data.iterrows()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to upsert our data, with only a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754ff510e75a472b97e474adaf922ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    kb.upsert(documents[i: i+batch_size])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the KnowledgeBase handle for use all the processing needed to load data into Pinecone. It chunks the text to smaller pieces and encode them to vectors (embeddings) that can be then upserted directly to Pinecone. Later in this notebook we'll learn how to tune and costumize this process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the KnowledgeBase\n",
    "\n",
    "Now we can query the knowledge base. The KnowledgeBase will use its default parameters like `top_k` to exectute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query_results(results):\n",
    "    for query_results in results:\n",
    "        print('query: ' + query_results.query + '\\n')\n",
    "        for document in query_results.documents:\n",
    "            print('document: ' + document.text.replace(\"\\n\", \"\\\\n\"))\n",
    "            print('source: ' + document.source)\n",
    "            print(f\"score: {document.score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: p1 pod capacity\n",
      "\n",
      "document: ### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.842927933\n",
      "\n",
      "document: ### p2 pods\\n\\n\\nThe p2 pod type provides greater query throughput with lower latency. For vectors with fewer than 128 dimension and queries where `topK` is less than 50, p2 pods support up to 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1.\\n\\n\\nEach p2 pod has enough capacity for around 1M vectors of 768 dimensions. However, capacity may vary with dimensionality.\\n\\n\\nThe data ingestion rate for p2 pods is significantly slower than for p1 pods; this rate decreases as the number of dimensions increases. For example, a p2 pod containing vectors with 128 dimensions can upsert up to 300 updates per second; a p2 pod containing vectors with 768 dimensions or more supports upsert of 50 updates per second. Because query latency and throughput for p2 pods vary from p1 pods, test p2 pod performance with your dataset.\\n\\n\\nThe p2 pod type does not support sparse vector values.\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.83271873\n",
      "\n",
      "document: ### Pod size and performance\\n\\n\\nPod performance varies depending on a variety of factors. To observe how your workloads perform on a given pod type, experiment with your own data set.\\n\\n\\nEach pod type supports four pod sizes: `x1`, `x2`, `x4`, and `x8`. Your index storage and compute capacity doubles for each size step. The default pod size is `x1`. You can increase the size of a pod after index creation.\\n\\n\\nTo learn about changing the pod size of an index, see [Manage indexes](manage-indexes/#changing-pod-sizes).\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.799543083\n",
      "\n",
      "document: ## Pods, pod types, and pod sizes\\n\\n\\nPods are pre-configured units of hardware for running a Pinecone service. Each index runs on one or more pods. Generally, more pods mean more storage capacity, lower latency, and higher throughput. You can also create pods of different sizes.\\n\\n\\nOnce an index is created using a particular pod type, you cannot change the pod type for that index. However, you can [create a new index from that collection](manage-indexes/#create-an-index-from-a-collection) with a different pod type.\\n\\n\\nDifferent pod types are priced differently. See [pricing](https://www.pinecone.io/pricing/) for more details.\\n\\n\\n### Starter plan\\n\\n\\nWhen using the starter plan, you can create one pod with enough resources to support approximately 100,000 vectors with 1536-dimensional embeddings and metadata; the capacity is proportional for other dimensions.\\n\\n\\nWhen using a starter plan, all [`create_index`](/reference/create_index) calls ignore the `pod_type` parameter.\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.791258693\n",
      "\n",
      "document: ---\\n\\n* [Table of Contents](#)\\n* + [Overview](#overview)\\n\t+ [Pods, pod types, and pod sizes](#pods-pod-types-and-pod-sizes)\\n\t\t- [Starter plan](#starter-plan)\\n\t\t- [s1 pods](#s1-pods)\\n\t\t- [p1 pods](#p1-pods)\\n\t\t- [p2 pods](#p2-pods)\\n\t\t- [Pod size and performance](#pod-size-and-performance)\\n\t\t- [Distance metrics](#distance-metrics)\n",
      "source: https://docs.pinecone.io/docs/indexes\n",
      "score: 0.786965072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\")])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use metadata filtering and specify `top_k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "KnowledgeBase is not connected to index canopy--my-index, Please call knowledge_base.connect(). ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcanopy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_models\u001b[39;00m \u001b[39mimport\u001b[39;00m Query\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m kb\u001b[39m.\u001b[39;49mquery([Query(text\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mp1 pod capacity\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                           metadata_filter\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mlimits\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      4\u001b[0m                           top_k\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)])\n\u001b[1;32m      6\u001b[0m print_query_results(results)\n",
      "File \u001b[0;32m~/canopy-quick/lib/python3.10/site-packages/canopy/knowledge_base/knowledge_base.py:432\u001b[0m, in \u001b[0;36mKnowledgeBase.query\u001b[0;34m(self, queries, global_metadata_filter)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39mQuery the knowledge base to retrieve document chunks.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m    >>> results = kb.query(queries)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_error_msg)\n\u001b[1;32m    434\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder\u001b[39m.\u001b[39mencode_queries(queries)\n\u001b[1;32m    435\u001b[0m results \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query_index(q, global_metadata_filter) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m queries]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: KnowledgeBase is not connected to index canopy--my-index, Please call knowledge_base.connect(). "
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Query\n",
    "results = kb.query([Query(text=\"p1 pod capacity\",\n",
    "                          metadata_filter={\"title\": \"limits\"},\n",
    "                          top_k=2)])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, using the metadata filter we get results only from the \"limits\" page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Context Engine\n",
    "\n",
    "While the `KnowledgeBase` is in charge of excuting a textual queries against the Pinecone index, `ContextEngine` is a higher level component that holds the KnowledgeBase, but have a slightly different API:\n",
    "\n",
    "1. The context engine can get user questions in natural langague. It then generate a search queries out of it. For example, given the question *\"What is the capacity of p1 pods?\"*, the ContextEngine would first convert it into the search query *\"p1 pod capacity\"* and then run it against the KnowledgeBase.\n",
    "2. The `query` method of context engine support a `max_context_tokens` that can limit the number of tokens used in its results. This capabillity allows the user to better handle tokens budgest and limit in the prompts sending later to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.context_engine import ContextEngine\n",
    "context_engine = ContextEngine(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"What is the capacity of p1 pods?\",\n",
      "  \"snippets\": [\n",
      "    {\n",
      "      \"source\": \"https://docs.pinecone.io/docs/indexes\",\n",
      "      \"text\": \"### s1 pods\\n\\n\\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\\n\\n\\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\\n\\n\\n### p1 pods\\n\\n\\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\\n\\n\\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"https://docs.pinecone.io/docs/indexes\",\n",
      "      \"text\": \"### p2 pods\\n\\n\\nThe p2 pod type provides greater query throughput with lower latency. For vectors with fewer than 128 dimension and queries where `topK` is less than 50, p2 pods support up to 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1.\\n\\n\\nEach p2 pod has enough capacity for around 1M vectors of 768 dimensions. However, capacity may vary with dimensionality.\\n\\n\\nThe data ingestion rate for p2 pods is significantly slower than for p1 pods; this rate decreases as the number of dimensions increases. For example, a p2 pod containing vectors with 128 dimensions can upsert up to 300 updates per second; a p2 pod containing vectors with 768 dimensions or more supports upsert of 50 updates per second. Because query latency and throughput for p2 pods vary from p1 pods, test p2 pod performance with your dataset.\\n\\n\\nThe p2 pod type does not support sparse vector values.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "# tokens in context returned: 415\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = context_engine.query([Query(text=\"What is the capacity of p1 pods?\", top_k=5)], max_context_tokens=512)\n",
    "\n",
    "print(result.to_text(indent=2))\n",
    "print(f\"\\n# tokens in context returned: {result.num_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we queried the context engine with a question in natural language. Also, even though we set `top_k=5`, context engine retreived only 3 results in order to satisfy the 512 tokens limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledgeable chat engine\n",
    "\n",
    "Now we are ready to start chatting with our data!\n",
    "\n",
    "Canopy `ChatEngine` supports OpenAI compatible API, only that behind the scenes it uses the context egine to provide knowledgeable answers to the users questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.chat_engine import ChatEngine\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each p1 pod has enough capacity for around 1 million vectors of 768 dimensions. [Source: Official Pinecone Documentation](https://docs.pinecone.io/docs/limits)\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import MessageBase\n",
    "\n",
    "response = chat_engine.chat(messages=[MessageBase(role=\"user\", content=\"What is the capacity of p1 pods?\")], stream=False)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Note: as opposed to OpenAI API, Canopy by default truncate the chat history to recent messages to avoid excceding the prompt tokens limit. This behaviour can change see chat engine [documentation](https://github.com/pinecone-io/canopy/blob/main/src/canopy/chat_engine/chat_engine.py)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costumization Example\n",
    "\n",
    "Canopy built as a modular library, where each component can fully be costumized by the user.\n",
    "\n",
    "Before we start, we would like to have a quick overview of the inner components used by the knowledge base:\n",
    "\n",
    "- **Index**: A Pinecone index that holds the vector representations of the documents.\n",
    "- **Chunker**: A `Chunker` object that is used to chunk the documents into smaller pieces of text.\n",
    "- **Encoder**: An `RecordEncoder` object that is used to encode the chunks and queries into vector representations.\n",
    "\n",
    "In the following example, we show how you can costumize the `Chunker` component used by the knowledge base.\n",
    "\n",
    "First, we will create a dummy chunker class that simply chunks the text by new lines `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from canopy.knowledge_base.chunker.base import Chunker\n",
    "from canopy.knowledge_base.models import KBDocChunk\n",
    "\n",
    "class NewLineChunker(Chunker):\n",
    "\n",
    "     def chunk_single_document(self, document: Document) -> List[KBDocChunk]:\n",
    "        line_chunks = [chunk\n",
    "                       for chunk in document.text.split(\"\\n\")]\n",
    "        return [KBDocChunk(id=f\"{document.id}_{i}\",\n",
    "                           document_id=document.id,\n",
    "                           text=text_chunk,\n",
    "                           source=document.source,\n",
    "                           metadata=document.metadata)\n",
    "                for i, text_chunk in enumerate(line_chunks)]\n",
    "    \n",
    "     async def achunk_single_document(self, document: Document) -> List[KBDocChunk]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBDocChunk(id='id1_0', text='This is first line', source='example', metadata={'title': 'newline'}, document_id='id1'),\n",
       " KBDocChunk(id='id1_1', text='This is the second line', source='example', metadata={'title': 'newline'}, document_id='id1')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = NewLineChunker()\n",
    "\n",
    "document = Document(id=\"id1\",\n",
    "                    text=\"This is first line\\nThis is the second line\",\n",
    "                    source=\"example\",\n",
    "                    metadata={\"title\": \"newline\"})\n",
    "chunker.chunk_single_document(document)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a new knowledge base to use our new chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(index_name=INDEX_NAME,\n",
    "                   chunker=chunker)\n",
    "kb.connect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And upsert our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.upsert([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: second line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = kb.query([Query(text=\"second line\",\n",
    "                          metadata_filter={\"title\": \"newline\"})])\n",
    "\n",
    "print_query_results(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, our knowledge base split the document by new line as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.delete_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canopy-quick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9b81017be88d4d093a2a92984a986685ce96a6b6736b12c233fdf6b743e185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
